{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790ca382",
   "metadata": {},
   "source": [
    "# Langgraph + DeepAgent + Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa649c22",
   "metadata": {},
   "source": [
    "# Tavily as external data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5856e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing YQL: select content_summary_chunk, url, tenant_id, file_id from sources pefund.summary where tenant_id contains \"90da21d0-8e83-4e84-961b-e6fec8b9dafe\" AND doc_type IN ('apollo')\n",
      "\n",
      "--- Node: gatherer ---\n",
      "Report Status: [{'type': 'text', 'text': \"Here's a detailed financial report for tenant '90da21d0-8e83-4e84-961b-e6fec8b9dafe'.\\n\\n**Summary of Raw Data:**\\n\\nThe report pertains to **PhonePe**, a company established in 2015 within the Information Technology & Services sector.\\n*   **Total Funding Raised**: $4,761,021,312\\n*   **Estimated Annual Revenue**: $604,000,000\\n\\nThe company has a detailed funding history, including:\\n*   **March 1, 2023**: A $200M funding round from Walmart.\\n*   **February 1, 2023**...\n",
      "✅ Report saved to final_financial_report_flash.md\n",
      "\n",
      "--- Node: reflector ---\n",
      "Report Status: Here's a detailed financial report for tenant '90da21d0-8e83-4e84-961b-e6fec8b9dafe'.\n",
      "\n",
      "**Summary of Financial Report: PhonePe**\n",
      "\n",
      "PhonePe, established in 2015 within the Information Technology & Services sector, is a leading fintech platform in India, dominating the UPI market. The company has demonstrated significant growth and is actively diversifying its revenue streams in preparation for a targeted Initial Public Offering (IPO).\n",
      "\n",
      "**Key Financial Highlights:**\n",
      "\n",
      "*   **Total Funding Raised**: Ph...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Annotated, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain / LangGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# DeepAgent & Vespa\n",
    "from deepagents import create_deep_agent\n",
    "from deepagents.backends import FilesystemBackend\n",
    "from vespa.application import Vespa\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- 1. TOOL DEFINITIONS ---\n",
    "\n",
    "@tool\n",
    "def execute_vespa_yql(yql_query: str):\n",
    "    \"\"\"Executes a YQL query against the Vespa AI index.\"\"\"\n",
    "    try:\n",
    "        app = Vespa(\n",
    "            url=os.getenv(\"VESPA_ENDPOINT\"), \n",
    "            cert=os.getenv(\"VESPA_CERT\"), \n",
    "            key=os.getenv(\"VESPA_KEY\")\n",
    "        )\n",
    "        print(f\"Executing YQL: {yql_query}\") \n",
    "        result = app.query(body={\"yql\": yql_query})\n",
    "        if not result.hits:\n",
    "            return \"No results found for this query.\"\n",
    "        return [hit['fields'] for hit in result.hits]\n",
    "    except Exception as e:\n",
    "        return f\"Vespa Error: {str(e)}\"\n",
    "\n",
    "# Corrected Tavily Integration for current LangChain versions\n",
    "tavily_tool = TavilySearchResults(api_key=os.getenv(\"TAVILY_API_KEY\"), k=5)\n",
    "\n",
    "SKILL_INSTRUCTION = \"\"\"\n",
    "You are a Deep Agent with access to a local filesystem and external database tools.\n",
    "LOCATION: Your skills are in './skills/'.\n",
    "\n",
    "PROTOCOL:\n",
    "1. EXPLORE: 'ls' on './skills' to find capabilities.\n",
    "2. LOAD: Read 'SKILL.md' and follow specific YQL patterns.\n",
    "3. QUERY: Use 'execute_vespa_yql' to fetch data.\n",
    "4. STORAGE: Save final findings as a .md file in the root workspace.\n",
    "5. SUMMARY: Provide a raw data summary for the next agent.\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. STATE DEFINITION ---\n",
    "\n",
    "class ReflexionState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], \"Conversation history\"]\n",
    "    internal_data: str  # Data captured from Vespa (Deep Agent result)\n",
    "    draft: str          # Current version of the report (Reflector output)\n",
    "    critique: str       # Feedback from Tavily/Audit\n",
    "    iteration: int\n",
    "\n",
    "# --- 3. NODE: DEEP AGENT (THE GATHERER) ---\n",
    "\n",
    "def deep_agent_gatherer(state: ReflexionState):\n",
    "    \"\"\"Gathers data from Vespa and hands it over to the Reflector.\"\"\"\n",
    "    working_dir = os.path.join(os.getcwd(), \"agent_workspace\")\n",
    "    fs_backend = FilesystemBackend(root_dir=working_dir, virtual_mode=True)\n",
    "    \n",
    "    # Keeping your specified model and temperature\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "    \n",
    "    executor = create_deep_agent(\n",
    "        model=llm,\n",
    "        backend=fs_backend,\n",
    "        tools=[execute_vespa_yql],\n",
    "        system_prompt=f\"You are a Financial Analyst agent.\\n{SKILL_INSTRUCTION}\"\n",
    "    )\n",
    "    \n",
    "    result = executor.invoke({\"messages\": state['messages']})\n",
    "    deep_agent_output = result[\"messages\"][-1].content\n",
    "    \n",
    "    return {\n",
    "        \"internal_data\": deep_agent_output,\n",
    "        \"draft\": deep_agent_output, \n",
    "        \"iteration\": 0\n",
    "    }\n",
    "\n",
    "# --- 4. NODE: THE REFLECTOR (CRITIQUE & FINAL REPORT) ---\n",
    "\n",
    "def reflexion_editor(state: ReflexionState):\n",
    "    \"\"\"Critiques using Tavily and generates the final refined report.\"\"\"\n",
    "    iteration = state.get(\"iteration\", 0)\n",
    "    current_draft = str(state['draft'])\n",
    "    internal_facts = state['internal_data']\n",
    "    \n",
    "    editor_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.7)\n",
    "    \n",
    "    # --- CHANGE: Generate a short query to avoid Tavily 400 Error ---\n",
    "    query_gen_prompt = f\"Based on this company data summary, generate a 10-word search query to find the latest 2025 financial news: {internal_facts}\"\n",
    "    response_content = editor_llm.invoke(query_gen_prompt).content\n",
    "    if isinstance(response_content, list):\n",
    "        search_query = response_content[0] if isinstance(response_content[0], str) else response_content[0].get(\"text\", str(response_content))\n",
    "    else:\n",
    "        search_query = str(response_content)\n",
    "    \n",
    "    # 1. Search for external validation using Tavily\n",
    "    tavily_results = tavily_tool.invoke({\"query\": search_query})\n",
    "    \n",
    "    # 2. Reflexion: Critique and Refine\n",
    "    refine_prompt = f\"\"\"\n",
    "    INTERNAL DATA (From Deep Agent/Vespa): \n",
    "    {internal_facts}\n",
    "    \n",
    "    EXTERNAL RESEARCH (From Tavily): \n",
    "    {tavily_results}\n",
    "    \n",
    "    CURRENT DRAFT: \n",
    "    {current_draft}\n",
    "    \n",
    "    Your Task:\n",
    "    1. Critique the draft: Does the internal Vespa data conflict with Tavily search results?\n",
    "    2. Rewrite the final report. Synthesize both sources into a professional financial analysis.\n",
    "    3. IMPORTANT: Return ONLY the final report content in Markdown format. Do NOT include any Python code to save the file. The system will save it for you.\n",
    "    4. If the report is complete, include '[FINAL_VERSION]' at the end.\n",
    "    \"\"\"\n",
    "    \n",
    "    refined_response = editor_llm.invoke(refine_prompt).content\n",
    "    if isinstance(refined_response, list):\n",
    "        refined_output = refined_response[0].get(\"text\", str(refined_response))\n",
    "    else:\n",
    "        refined_output = str(refined_response)\n",
    "    final_report = refined_output\n",
    "    if \"---\" in refined_output:\n",
    "        # If the LLM provides Critique --- Report, we take the last part\n",
    "        parts = refined_output.split(\"---\")\n",
    "        final_report = parts[-1].strip()\n",
    "\n",
    "    # --- SAVE TO FILE ---\n",
    "    file_path = \"final_financial_report_flash.md\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_report)\n",
    "        print(f\"✅ Report saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving file: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"draft\": final_report,\n",
    "        \"critique\": \"Draft updated with Tavily research.\",\n",
    "        \"iteration\": iteration + 1\n",
    "    }\n",
    "\n",
    "# --- 5. GRAPH CONSTRUCTION ---\n",
    "\n",
    "def should_loop(state: ReflexionState):\n",
    "    if \"[FINAL_VERSION]\" in state['draft'] or state['iteration'] >= 3:\n",
    "        return END\n",
    "    return \"reflector\"\n",
    "\n",
    "builder = StateGraph(ReflexionState)\n",
    "\n",
    "builder.add_node(\"gatherer\", deep_agent_gatherer)\n",
    "builder.add_node(\"reflector\", reflexion_editor)\n",
    "\n",
    "builder.set_entry_point(\"gatherer\")\n",
    "builder.add_edge(\"gatherer\", \"reflector\")\n",
    "builder.add_conditional_edges(\"reflector\", should_loop)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# --- 6. EXECUTION ---\n",
    "\n",
    "\n",
    "user_task = \"Prepare a detailed financial report for tenant '90da21d0-8e83-4e84-961b-e6fec8b9dafe'.\"\n",
    "inputs = {\"messages\": [HumanMessage(content=user_task)]}\n",
    "\n",
    "for event in graph.stream(inputs):\n",
    "    for node, values in event.items():\n",
    "        print(f\"\\n--- Node: {node} ---\")\n",
    "        if \"draft\" in values:\n",
    "            print(f\"Report Status: {str(values['draft'])[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6897b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except ImportError:\n",
    "    print(\"IPython is not available. Skipping graph display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181effe",
   "metadata": {},
   "source": [
    "# Gemini 3 flash preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0faac1",
   "metadata": {},
   "source": [
    "# Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f69a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_DUE_DILIGENCE_PROMPT_COMPACT = \"\"\"\n",
    "[SYSTEM DIRECTIVE: ACT AS SENIOR IC MEMBER - DUE DILIGENCE HEAD]\n",
    "You are conducting final IC review for a multi-billion dollar acquisition. Tone: skeptical, data-driven, LP-capital protective.\n",
    "\n",
    "**TARGET:** {header_title}\n",
    "**DATA:** ---\n",
    "{chunk_content}\n",
    "---\n",
    "\n",
    "**OBJECTIVE:**\n",
    "Generate 4-5 \"Surgical Strike\" questions that dismantle management narrative and expose truth behind GAAP/IFRS numbers.\n",
    "\n",
    "**FOCUS AREAS (prioritize most critical):**\n",
    "1. **QoE & Accrual Games:** Non-cash padding, provision releases (Zakat/Tax), one-time gains masking decay\n",
    "2. **Capital Intensity:** PP&E vs Depreciation gaps, deferred CAPEX bow waves, maintenance underspend\n",
    "3. **Liquidity Risk:** Current liabilities vs cash, trade payables as shadow financing\n",
    "4. **FX/Macro Exposure:** SAR/USD peg stress, translation losses, debt-to-equity under de-peg scenarios\n",
    "5. **Structural Leakage:** NCI dividends vs parent retained earnings pressure\n",
    "\n",
    "**RULES:**\n",
    "- Max 1-2 sentences per question\n",
    "- MUST cite specific SAR values + YoY changes\n",
    "- Use advanced metrics: ROIC, FCF Conversion, Working Capital Intensity\n",
    "- Zero generic questions\n",
    "- Assume management hides underperformance in \"Other\" line items\n",
    "\n",
    "**OUTPUT:** 4-5 razor-sharp questions only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5eab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting LangGraph Audit Workflow ---\n",
      "--- Fetching and Summarizing Vespa Context for Tenant: 0f7a4b5f-a137-4a71-b518-1a04ba239b61 ---\n",
      "✅ Entity Profile Created.\n",
      "\n",
      "[NODE 1] Researching Section: CONSOLIDATED STATEMENT OF FINANCIAL POSITION\n",
      "[NODE 2] Reflecting on: CONSOLIDATED STATEMENT OF FINANCIAL POSITION\n",
      "\n",
      "[SUCCESS] Internal Report: outputs/output_md_files/internal_search_report.md\n",
      "[SUCCESS] Verified Report: outputs/output_md_files/final_verified_audit_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import httpx\n",
    "import json\n",
    "import time\n",
    "from typing import List, Annotated, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from pydantic import BaseModel, Field\n",
    "from vespa.application import Vespa\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ======================================================\n",
    "# 1. Configuration & State Definition\n",
    "# ======================================================\n",
    "\n",
    "API_URL = \"http://localhost:8000/api/chat\"\n",
    "TENANT_ID = \"0f7a4b5f-a137-4a71-b518-1a04ba239b61\" \n",
    "INPUT_FILE = \"input/sample_table.md\"\n",
    "\n",
    "# Output Filenames\n",
    "INTERNAL_MD_FILE = \"outputs/output_md_files/internal_search_report.md\"\n",
    "FINAL_VERIFIED_MD_FILE = \"outputs/output_md_files/final_verified_audit_report.md\"\n",
    "\n",
    "vespa_app = Vespa(\n",
    "    url=os.getenv(\"VESPA_ENDPOINT\"),\n",
    "    cert=os.getenv(\"VESPA_CERT\"),\n",
    "    key=os.getenv(\"VESPA_KEY\")\n",
    ")\n",
    "\n",
    "class ReflexionState(TypedDict):\n",
    "    sections: List[dict]           \n",
    "    current_index: int             \n",
    "    internal_results: List[dict]   \n",
    "    final_report_data: List[dict]  \n",
    "    company_context: str           \n",
    "    iteration_count: int\n",
    "\n",
    "class QuestionList(BaseModel):\n",
    "    questions: List[str] = Field(description=\"List of due diligence questions\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    temperature=0.6, \n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "structured_qa_model = llm.with_structured_output(QuestionList)\n",
    "perplexity_tool = ChatPerplexity(pplx_api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# ======================================================\n",
    "# 2. Node Logic\n",
    "# ======================================================\n",
    "\n",
    "def initial_setup_node(state: ReflexionState):\n",
    "    \"\"\"NODE 0: Fetches Vespa Chunks and Summarizes them into a Fact Sheet.\"\"\"\n",
    "    print(f\"--- Fetching and Summarizing Vespa Context for Tenant: {TENANT_ID} ---\")\n",
    "    \n",
    "    query = {\n",
    "        \"yql\": (\n",
    "            \"select content_summary_chunk from sources pefund.summary \"\n",
    "            f\"where tenant_id contains '{TENANT_ID}' \"\n",
    "            \"AND doc_type IN ('apollo')\"\n",
    "        ),\n",
    "        \"hits\": 6,\n",
    "    }\n",
    "    result = vespa_app.query(body=query)\n",
    "    raw_chunks = [hit['fields']['content_summary_chunk'] for hit in result.hits if 'content_summary_chunk' in hit['fields']]\n",
    "    combined_raw = \"\\n\\n\".join(raw_chunks)\n",
    "\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize these company data chunks into a single, dense paragraph (max 150 words).\n",
    "    Focus on: Company Name, HQ, Year Founded, Business Model, and Key Investors.\n",
    "    RAW DATA: {combined_raw}\n",
    "    CLEAN SUMMARY:\n",
    "    \"\"\"\n",
    "    clean_summary = llm.invoke(summary_prompt).content.strip()\n",
    "    print(f\"✅ Entity Profile Created.\")\n",
    "    return {\"company_context\": clean_summary}\n",
    "\n",
    "def question_generator_node(state: ReflexionState):\n",
    "    \"\"\"NODE 1: Generates questions and fetches internal API responses.\"\"\"\n",
    "    idx = state[\"current_index\"]\n",
    "    section = state[\"sections\"][idx]\n",
    "    header = section.metadata.get(\"Header_Title\", \"Financial Section\")\n",
    "    \n",
    "    print(f\"\\n[NODE 1] Researching Section: {header}\")\n",
    "    \n",
    "    # Placeholder for your specific prompt logic\n",
    "    formatted_prompt = PE_DUE_DILIGENCE_PROMPT_COMPACT.format(\n",
    "        header_title=header,\n",
    "        chunk_content=section.page_content\n",
    "    )\n",
    "    response_obj = structured_qa_model.invoke(formatted_prompt)\n",
    "    questions = response_obj.questions\n",
    "    \n",
    "    internal_findings = []\n",
    "    session_id = str(uuid.uuid4())\n",
    "\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        payload = {\n",
    "            \"query\": q, \"session_id\": session_id, \"message_id\": str(i),\n",
    "            \"web_search\": False, \"conversations\": [], \"tenant_ids\": [TENANT_ID]\n",
    "        }\n",
    "        try:\n",
    "            with httpx.Client(timeout=240.0) as client:\n",
    "                api_resp = client.post(API_URL, json=payload)\n",
    "                if api_resp.status_code == 200:\n",
    "                    clean_res = api_resp.json().get(\"response\", \"No response found.\")\n",
    "                    internal_findings.append({\"question\": q, \"answer\": clean_res})\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "\n",
    "    # --- SAVE NODE 1 RESULTS SEPARATELY ---\n",
    "    with open(INTERNAL_MD_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n## Research Section: {header}\\n\")\n",
    "        for item in internal_findings:\n",
    "            f.write(f\"### Q: {item['question']}\\n**Internal Result:** {item['answer']}\\n\\n\")\n",
    "\n",
    "    return {\"internal_results\": internal_findings}\n",
    "\n",
    "def reflection_agent_node(state: ReflexionState):\n",
    "    \"\"\"NODE 2: Audits using Perplexity but saves the ORIGINAL question in the report.\"\"\"\n",
    "    internal_results = state[\"internal_results\"]\n",
    "    idx = state[\"current_index\"]\n",
    "    header = state[\"sections\"][idx].metadata.get(\"Header_Title\", \"Section\")\n",
    "    clean_profile = state[\"company_context\"]\n",
    "    \n",
    "    print(f\"[NODE 2] Reflecting on: {header}\")\n",
    "    final_section_results = []\n",
    "    \n",
    "    # Open the final report in append mode once for the section\n",
    "    with open(FINAL_VERIFIED_MD_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n## Verified Audit: {header}\\n\")\n",
    "        \n",
    "        for item in internal_results:\n",
    "            # We explicitly use the original question from the internal results\n",
    "            original_question = item[\"question\"]\n",
    "            internal_answer = item[\"answer\"]\n",
    "            \n",
    "            # 1. Synthesis of Search Query (Not saved, only used for research)\n",
    "            INVESTIGATIVE_PROMPT_TEMPLATE = \"\"\"\n",
    "            [ROLE: Forensic Auditor]\n",
    "            Company Profile: {clean_profile}\n",
    "            Internal Claim: {internal_answer}\n",
    "            \n",
    "            Generate a high-precision, investigational search query for Perplexity \n",
    "            to identify discrepancies or financial actuals.\n",
    "            INVESTIGATIVE SEARCH QUERY:\n",
    "            \"\"\"\n",
    "            query_gen_prompt = INVESTIGATIVE_PROMPT_TEMPLATE.format(\n",
    "                clean_profile=clean_profile,\n",
    "                internal_answer=internal_answer  # FIXED: was initial_answer\n",
    "            )\n",
    "            search_query = llm.invoke(query_gen_prompt).content.strip()\n",
    "            \n",
    "            # 2. Perplexity Investigation\n",
    "            pplx_context = perplexity_tool.invoke(search_query).content\n",
    "            \n",
    "            # 3. Final Audit Synthesis\n",
    "            reflection_prompt = f\"\"\"\n",
    "            You are a Senior Private Equity Auditor.\n",
    "            COMPANY: {clean_profile}\n",
    "            INTERNAL DATA: {internal_answer}\n",
    "            EXTERNAL EVIDENCE (Perplexity): {pplx_context}\n",
    "            \n",
    "            Task: Provide a finalized, high-conviction answer. Correct any inaccuracies.\n",
    "            End with [FINAL_VERSION].\n",
    "            \"\"\"\n",
    "            final_verified_answer = llm.invoke(reflection_prompt).content\n",
    "            \n",
    "            # 4. SAVE TO FILE: Using original_question\n",
    "            f.write(f\"### Q: {original_question}\\n**Audited Result:** {final_verified_answer}\\n\\n\")\n",
    "            \n",
    "            final_section_results.append({\n",
    "                \"question\": original_question,\n",
    "                \"verified_answer\": final_verified_answer\n",
    "            })\n",
    "\n",
    "    # Accumulate data for state\n",
    "    updated_final_data = state.get(\"final_report_data\", [])\n",
    "    updated_final_data.append({\"section\": header, \"results\": final_section_results})\n",
    "\n",
    "    return {\n",
    "        \"final_report_data\": updated_final_data,\n",
    "        \"current_index\": state[\"current_index\"] + 1,\n",
    "        \"iteration_count\": state.get(\"iteration_count\", 0) + 1\n",
    "    }\n",
    "# ======================================================\n",
    "# 3. Graph Construction\n",
    "# ======================================================\n",
    "\n",
    "\n",
    "def should_continue(state: ReflexionState):\n",
    "    iteration_count = state.get(\"iteration_count\", 0)\n",
    "    \n",
    "    # Check both section completion and iteration limit\n",
    "    if iteration_count >= 3:\n",
    "        print(f\"[WARNING] Max iterations ({3}) reached. Stopping.\")\n",
    "        return END\n",
    "    \n",
    "    if state[\"current_index\"] < len(state[\"sections\"]):\n",
    "        return \"question_generator\"\n",
    "    \n",
    "    return END\n",
    "\n",
    "builder = StateGraph(ReflexionState)\n",
    "builder.add_node(\"setup\", initial_setup_node)\n",
    "builder.add_node(\"question_generator\", question_generator_node)\n",
    "builder.add_node(\"reflector\", reflection_agent_node)\n",
    "\n",
    "builder.add_edge(START, \"setup\")\n",
    "builder.add_edge(\"setup\", \"question_generator\")\n",
    "builder.add_edge(\"question_generator\", \"reflector\")\n",
    "builder.add_conditional_edges(\"reflector\", should_continue)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ======================================================\n",
    "# 4. Execution Logic\n",
    "# ======================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Clear existing files to start fresh\n",
    "    for f_path in [INTERNAL_MD_FILE, FINAL_VERIFIED_MD_FILE]:\n",
    "        if os.path.exists(f_path):\n",
    "            os.remove(f_path)\n",
    "    \n",
    "    with open(INTERNAL_MD_FILE, \"w\") as f: f.write(\"# Internal Research Report (Before Critique)\\n\")\n",
    "    with open(FINAL_VERIFIED_MD_FILE, \"w\") as f: f.write(\"# Final Verified Audit Report (After Critique)\\n\")\n",
    "\n",
    "    # 2. Prepare Sections\n",
    "    if os.path.exists(INPUT_FILE):\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            md_content = f.read()\n",
    "        \n",
    "        splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"###\", \"Header_Title\")])\n",
    "        sections = splitter.split_text(md_content)\n",
    "\n",
    "        initial_state = {\n",
    "            \"sections\": sections,\n",
    "            \"current_index\": 0,\n",
    "            \"internal_results\": [],\n",
    "            \"final_report_data\": [],\n",
    "            \"company_context\": \"\",\n",
    "            \"iteration_count\": 0\n",
    "        }\n",
    "\n",
    "        print(\"--- Starting LangGraph Audit Workflow ---\")\n",
    "        final_output = graph.invoke(initial_state)\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] Internal Report: {INTERNAL_MD_FILE}\")\n",
    "        print(f\"[SUCCESS] Verified Report: {FINAL_VERIFIED_MD_FILE}\")\n",
    "    else:\n",
    "        print(f\"Error: {INPUT_FILE} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3d4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
